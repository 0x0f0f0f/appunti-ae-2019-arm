\chapter{Microarchitettura}

\section{Datapath}

Il ciclo di esecuzione di un processore è

\begin{lstlisting}[frame=single]
while(true) {
    Instruction = fetch(PC) // PC = program counter
    decode(Instruction)
    exec(Instruction)
    update(PC)
}
\end{lstlisting}

Vedremo come implementare un piccolo processore che esegue un sottoinsieme delle
istruzioni ARM, suddiviso in due oggetti, la parte di controllo e il
\textbf{datapath} (parte operativa).

I processori possono essere di diversi tipi:
\begin{itemize}
    \item \textbf{Single cycle}: esegue un singolo ciclo fetch-decode-execute
    per ogni ciclo di clock (tutto viene eseguito nell'intervallo di tempo in
    cui il segnale di clock è basso).
    % //TODO schema
    \item \textbf{Multi cycle}: può eseguire un istruzione in più cicli di
    clock, in genere, un ciclo per il fetch-decode, un ciclo per l'exec e un
    ciclo per il writeback (risposta)
    % //TODO schema
    \item \textbf{Pipeline}
\end{itemize}

\begin{note}
    \textbf{Prestazioni}

    La misura \textit{CPI} (Clock per Instruction) misura quanti cicli di clock
    $\tau$ sono necessari per eseguire un istruzione. Da tale misura possiamo
    dedurre, per ogni processore, il tempo necessario per eseguire un programma
    di $N$ istruzioni. Esso impiegherà $N \cdot CPI \cdot \tau $
\end{note}


\section{Processori Single Cycle}

Per realizzare un processore Single Cycle dobbiamo capire quali componenti (reti
logiche e sequenziali) sono necessari per realizzare il datapath. Possiamo
inferire da i componenti necessari per mantenere lo stato del processore
(Registri e memoria) e l'insieme \textit{ISTR} di istruzioni che vogliamo
implementare.

Vediamo i \textbf{componenti di stato}, il primo componente da utilizzare è una
memoria per le istruzioni che riceverà in input un indirizzo e restituirà in
output l'\textbf{istruzione corrente}. Il secondo componente necessario è una
memoria dati (una RAM statica) che contiene la memoria sulla quale possiamo fare
operazioni di \textit{load} e \textit{store}. Ha bisogno di un solo indirizzo di
memoria per la lettura e la scrittura, un input di clock, un input di
\textit{write enable}, un indirizzo in input, un valore in input e un valore in
output.

Il terzo componente necessario è una memoria multiporta statica che continene lo
stato dei registri. Riceverà in input 3 indirizzi (2 in lettura ed 1 in
scrittura), un segnale di clock, uno di write enable, un valore in input e due
in output. Se il segnale \textit{write enable} è LOW, l'indirizzo in scrittura
viene utilizzato per la lettura. Sarà necessario un registro separato per
mantenere il program counter, che riceverà sempre clock, write enable, input e
restituirà il suo contenuto in output.

% //TODO schema dettagliato //TODO schema semplificato

\begin{defn}
\textbf{Implementare un'istruzione LDR (Load register)} \\
Vediamo come implementare un'istruzione \texttt{LDR} con offset immediato
(pre-indice), prendiamo ad esempio
\begin{lstlisting}[style=arm]
ldr r0, [r1, #4]
\end{lstlisting}
Avremo il registro \texttt{pc} che punterà all'istruzione \texttt{ldr}. La
memoria istruzioni conterrà l'istruzione all'indirizzo puntato da \texttt{pc}.
Caricheremo \texttt{r1} dal primo indirizzo di lettura della \textit{memoria
registri} (nel suo output 1), a cui sommeremo  nella \textit{ALU} l'offset
costante \texttt{\#4}. Caricheremo dall'indirizzo sommato un valore dalla
\textit{memoria dati}, che andrà in input alla memoria registri e verrà scritto
all'indirizzo di scrittura, in questo caso \texttt{r0}.
% //TODO SCHEMA
Se volessimo realizzare un offset variabile (un registro), come ad esempio
\begin{lstlisting}[style=arm]
ldr r0, [r1, r2]
\end{lstlisting}
Dovremmo utilizzare anche il secondo input/output di lettura della memoria
registri, (il registro \texttt{r2}) come operando di somma della ALU. Ciò ci fa
notare che è necessario un multiplexer fra \textit{out2} della memoria registri
e l'operando immediato per realizzare correttamente l'offset.
\end{defn}


\begin{defn}
\textbf{Implementare un'istruzione STR (Store register)} \\
Implementare un'istruzione di \textit{store} è simile all'implementazione di un
istruzione di \textit{load}. Il segnale \textit{write enable} della memoria
registri sarà low. Leggerò dal primo output della memoria registri l'indirizzo
in cui memorizzerò il valore, dal secondo output della memoria registri il
valore da memorizzare e opzionalmente, un registro di offset dal terzo output.
Il segnale \textit{write enable} della memoria dati sarà HIGH.
% //TODO ordine registri nella thumb
\end{defn}

\begin{defn}
\textbf{Implementare istruzioni di salto} \\
Per implementare le istruzioni di salto dobbiamo sommare un immediato
all'indirizzo corrente contenuto nel program counter, ed inserirlo di nuovo
all'interno del program counter. Abbiamo bisogno di un multiplexer posto fra
l'uscita della memoria dati e l'uscita della ALU posta dopo gli output della
memoria registri. Collegheremo l'uscita di tale multiplexer all'ingresso del
program counter, dove scriveremo il valore dell'istruzione dopo il salto.

Alla fine di un'istruzione \textit{non di salto} il program counter viene
incrementato di 4 posizioni attraverso una ALU. Ciò ci dice che è necessario
avere un altro multiplexer in ingresso al program counter.
% //TODO
\end{defn}

\section{Realizzazione di un Datapath in Verilog}

\centerfig{1}{completesinglecycleprocessor.png}{Processore a ciclo singolo completo}

\centerfig{1}{datapath_quartus}{Datapath visualizzato in Quartus}

\includecode[verilog]{./verilog/datapath/dp.v}{Modulo Datapath}

\includecode[verilog]{./verilog/datapath/mux2.v}{Multiplexer da 2 linee da 32 bit}

\includecode[verilog]{./verilog/datapath/registro.v}{Registro da 32 bit}

\includecode[verilog]{./verilog/datapath/rom.v}{Memoria delle istruzioni READ ONLY.}

\includecode[verilog]{./verilog/datapath/alu4.v}{ALU che incrementa di 4 per PC}

\includecode[verilog]{./verilog/datapath/regfile.v}{File dei registri}

\includecode[verilog]{./verilog/datapath/alupiumeno.v}{ALU principale}

\includecode[verilog]{./verilog/datapath/m.v}{Memoria Principale}

\includecode[verilog]{./verilog/datapath/extend.v}{Extend degli immediati}

\includecode[verilog]{./verilog/datapath/test_dp.v}{Test bench del Datapath}

\section{Processore a ciclo multiplo}

\centerfig{1}{multicycleprocessor.png}{Processore a ciclo multiplo completo}

\begin{note}
    \textbf{Svantaggi del processore a ciclo singolo} \\
    Un processore a ciclo singolo ha 3 svantaggi degni di nota:
    \begin{enumerate}
        \item Richiede memorie separate per le istruzioni ed i dati, laddove i
        processori moderni hanno una singola memoria esterna al processore dove
        sono contenuti entrambi.
        \item Richiede un ciclo di clock lungo abbastanza per supportare
        l'operazione più lenta: \texttt{LDR}, anche se le altre istruzioni
        potrebbero essere molto più veloci.
        \item Richiede tre addizionatori, uno nella \texttt{ALU} principale e
        due per la logica del \textit{program counter}.
    \end{enumerate}
\end{note}

\begin{defn}
    \textbf{Processore a multiciclo} \\
    Un processore a ciclo multiplo risolve questi problemi separando
    un'istruzione in diversi passaggi più piccoli da eseguire ognuno in un ciclo
    di clock, e unendo la memoria dati e la memoria istruzioni. In ogni
    passaggio piccolo (\textit{short step}) il processore può leggere o scrivere
    la memoria o il register file, o utilizzare la ALU. \@ L'istruzione corrente
    viene letta in un passaggio, e i dati possono essere letti o scritti in un
    passaggio successivo, rendendo possibile l'utilizzo di una singola memoria.
    Per rendere un processore multiciclo, introduciamo dei registri aggiuntivi
    nel \textit{datapath}, uno posto dopo le due uscite dei registri in lettura
    del file dei registri, per memorizzare il contenuto di essi ed un registro
    posto dopo la memoria. In tale modo è possibile separare un'istruzione in
    più cicli di clock. Vediamo un esempio per l'istruzione \texttt{ldr}:

    \begin{enumerate}
        \item \texttt{fetch} dell'istruzione dalla memoria.
        \item Lettura del registro da cui caricare, \texttt{decode}.
        \item Applicazione dell'offset tramite la \textit{ALU}.
        \item \texttt{load} dalla memoria di un valore (memorizzato in un
        ulteriore registro aggiuntivo).
        \item Memorizzazione del valore letto dalla memoria nel registro
        destinazione.
    \end{enumerate}

    Per utilizzare soltanto una memoria per istruzioni e dati, abbiamo bisogno
    di introdurre un multiplexer di fronte alla memoria, che controllerà se
    l'indirizzo in lettura della memoria sarà dettato dal program counter o dal
    risultato dell'operazione precedente. Ciò necessita dell'introduzione di
    memoria nella \textit{parte di controllo} del processore, ovvero va resa la
    parte di controllo un automa.

    Nonostante in un processore multiciclo si impieghi leggermente di più per
    eseguire un'istruzione di memoria, ne otterremo che per realizzare
    istruzioni operative occorrono meno cicli di clock delle istruzioni di
    memoria, rendendo effettivamente più efficace e rapido il processore.
\end{defn}

\begin{figure}[H]
    \centering

    \caption{Esempio di della parte di controllo di un processore multiciclo. Vediamo soltanto le istruzioni \texttt{ldr} e \texttt{add} per semplicità}
    \label{fig:pcmulticycle}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm]
	
	\node[state,initial] 	(A)                    {\texttt{fetch}}; \node[state]
	(B) [below right of=A] 	   {\texttt{add} (\texttt{decode})}; \node[state]
	(D) [below of=B] 	   {\texttt{add} (\texttt{ALU})}; \node[state] (E) [below
	of=D] 	   {\texttt{add} (\texttt{reg})}; \node[state] (C) [below left of=A]
	{\texttt{ldr} (\texttt{decode})}; \node[state] (F) [below of=C]
	{\texttt{ldr} (\texttt{offset})}; \node[state] (G) [below of=F]
	{\texttt{ldr} (\texttt{load})}; \node[state] (H) [below of=G] {\texttt{ldr}
	(\texttt{reg})};

    \path
    (A)		edge [] node [above right] {\makecell[l]{$OP = 00$}} 		(B) edge [] node
		    [above left] {\makecell[l]{$OP = 01$}} 	    (C) (B)     edge [] node
		    {} (D) (D)     edge [] node {} (E) (C)     edge [] node {} (F) (F)
		    edge [] node {} (G) (G)     edge [] node {} (H) ;
    \end{tikzpicture}
    % TODO fixa label
\end{figure}

\section{Sottosistema di Memoria}
Prima di realizzare un \textit{processore pipelined} dobbiamo fornire più
dettagli sul sottosistema di memoria.

\begin{defn}
    \textbf{Cache} \\
    Abbiamo diversi tipi di memorie, oltre a quella generale (dati/istruzioni),
    è presenta una memoria "\textit{più vicina al processore}", detta
    \textbf{cache}. Le cache sono estremamente rapide, il tempo di accesso è
    all'incirca uno o due cicli di clock nei processori moderni, ma presentano
    lo svantaggio di essere molto costose.
\end{defn}

\begin{defn}
    \textbf{Gerarchia di memoria} \\
    Le memorie possono essere di diversi livelli. Il livello più basso è il "più
    vicino al processore", ovvero il più rapido. In generale i primi livelli
    sono occupati dalle memorie \textit{cache}. Il livello più alto delle
    memorie volatili è invece occupato dalla RAM, ovvero la memoria centrale
    (che non è una cache). Il livello assolutamente più in alto è occupato dalle
    memorie permanenti o memorie di massa, ovvero dischi rigidi o a stato solido
    (rispettivamente \textit{hard disk} e \textit{SSD}). HDD e SSD sono molto
    capienti, ma non li utilizziamo come memoria volatile per istruzioni e dati
    di programmi in esecuzione perché i loro tempi di accesso sono molto alti
    rispetto alle memorie cache e le memorie RAM. Li utilizziamo invece per
    memorizzare permanentemente (anche dopo lo spegnimento della macchina) file
    e dati. RAM e cache vengono invece completamente cancellate al momento dello
    spegnimento della macchina e sono rispettivamente molto meno capienti dei
    moderni metodi di archiviazione di massa.
\end{defn}

\begin{exmp}
    \textbf{Caricamento di un programma dal disco rigido} \\
    Supponiamo di scrivere un programma asm ARMv7. Utilizzeremo un editor di
    testo e \textit{salveremo il file} su memoria di massa. Il file avrà un
    percorso denotato dalla radice del \textit{filesystem}, seguita dalla
    sequenza di directory in cui è contenuto il file, ad esempio
    \texttt{/home/studente/assembler/programma.s} (le memorie di massa hanno
    metodi di indirizzamento dell'accesso dei contenuti). Compileremo poi il
    programma con \texttt{gcc}, che caricherà la rappresentazione testuale del
    programma in memoria volatile (il file \texttt{.s}), genererà un file
    binario eseguibile sempre in memoria RAM e lo scriverà di nuovo in formato
    eseguibile sul disco rigido. A momento di esecuzione il programma appena
    compilato viene caricato in memoria RAM nella sezione delle istruzioni, che
    continueranno a "scendere" nella gerarchia delle memorie fino ad essere
    eseguite direttamente nei registri del processore.
\end{exmp}

\begin{defn}
    \textbf{Cache di primo e secondo livello} \\
    La cache di primo livello è la più vicina al processore e generalmente ha
    dimensioni nell'ordine di decine di \texttt{KB}. La cache di secondo livello
    è leggermente più lenta e ha dimensioni nell'ordine di \texttt{MB}. La RAM,
    ha nelle macchine moderne dimensioni dell'ordine di \texttt{GB}. Le cache
    sono memorie SRAM e sono molto più costose rispetto alle DRAM.
\end{defn}

\begin{defn}
    \textbf{Hit e miss} \\
    Definiamo due concetti necessari per il funzionamento delle cache,
    \textbf{hit e miss}. Il primo indica cercare una locazione di memoria nella
    cache e trovarla, mentre il secondo indica non trovarla. Definiamo due
    probabilità complementari fra di loro: $p_h$ e $p_m$, che indicano la
    percentuale di accessi in memoria con esito \textit{hit} $p_h$, ovvero
    \textbf{hit rate} e \textit{miss} $p_m$, \textbf{miss rate}. Supponiamo di
    avere una gerarchia di memoria con due tre livelli (processore, cache e
    RAM), le probabilità definite per la cache e dei tempi di accesso per la
    cache e la memoria $ta_c$ e $ta_m$. Per accedere ad una locazione di
    memoria, se abbiamo un cache hit il tempo di accesso sarà $ta_h$, se avremo
    un cache miss significa che abbiamo cercato prima nella cache e
    successivamente è stato necessario l'accesso alla memoria generale. Avremo
    che il tempo medio di accesso ad una locazione di memoria è
    \begin{equation*}
        \begin{aligned}
            ta = p_h ta_c + (1-p_h)ta_m
        \end{aligned}
    \end{equation*}
\end{defn}

\begin{defn}
    \textbf{Località spaziale} \\
    La località spaziale indica che è probabile accedere a locazioni di memoria
    fisicamente vicine ad una di quelle in cui viene effettuato l'accesso. Se al
    tempo $t$ il processore accede all'indirizzo $x$ è probabile che vada ad
    accedere al tempo $t+a$ all'indirizzo $x+1$, con $a$ relativamente piccolo.
\end{defn}

\begin{defn}
    \textbf{Località temporale} \\
    Se il processore ha acceduto ad una locazione di memoria ad un tempo $t$, è
    probabile che ci acceda un'altra volta ad un tempo $t+a$ con $a$
    relativamente piccolo.
\end{defn}

\begin{defn}
    \textbf{Working Set} \\
    Insieme dei dati e del codice necessari al funzionamento di un programma in
    un certo lasso di tempo.
\end{defn}

\begin{note}
    La presenza di una cache deve essere trasparente al processore. Il datapath
    del processore deve poter accedere ai contenuti memorizzati nella cache in
    maniera identica a quella con cui accede alla memoria generale.
\end{note}

\begin{defn}
    \textbf{Funzionamento di una cache} \\
    Le cache implementano un dizionario \textit{chiave-valore}. Dove la chiave è
    l'indirizzo di memoria e il valore è il contenuto della locazione di memoria
    a quell'indirizzo. Per venire incontro al principio di località, ogni volta
    che accedo ad un indice di memoria $x$ posso caricare nella cache un
    "intorno" di memoria di $x$ (zz)
\end{defn}

\begin{defn}
    \textbf{Linea di cache} \\
    Un \textbf{blocco} di parole (\textit{linea di cache} o \textit{blocco di
    cache}) è una sezione della cache utilizzata per sfruttare il principio di
    località. Il numero di parole in un set è detto $b$. Una cache di capacità
    $C$ continene $B=C/b$ blocchi.
\end{defn}

\begin{exmp}
    Supponiamo di avere una memoria con indirizzi da 8 bit (256 byte, 64 parole
    da 32 bit), e una cache con $2^4 = 16$ insiemi. Se il processore esegue
    un'istruzione per caricare dalla memoria un valore all'indirizzo $x$, la
    cache caricherà dalla memoria i valori all'indirizzo $x$ e tutte le altre
    combinazioni degli ultimi $\log_2(b)$ bit dell'indirizzo di $x$. Ad esempio,
    se il processore intende caricare un valore all'indirizzo $10001100$ (8
    bit), nella cache esisterà un \textbf{blocco} con numero di insieme $1000$
    (4 bit, esclusi gli ultimi quattro, due per l'offset dei byte e due per le
    parole) che conterrà 4 parole di memoria, ovvero le parole contenute nella
    memoria generale agli indirizzi $100000$, $100001$, $100010$ $100011$ (tutte
    le parole nell'intorno dell'indirizzo $x$).
\end{exmp}

\begin{defn}
    \textbf{Cache ad indirizzamento diretto} \\
    Considerando l'esempio soprastante, nell'architettura ARMv7 avremmo sempre
    indirizzi di memoria da 32 bit e non da 8 bit. Ci accorgiamo facilmente che
    in un esempio reale una cache difficilmente avrà un numero di blocchi $B$
    esattamente uguale alle possibili combinazioni di bit di un indirizzo di
    memoria rimuovendo 2 bit per il \textit{byte offset} e $\log_2(b)$ bit per
    le parole contenute nel set. In una cache ad indirizzamento diretto, abbiamo
    generalmente meno insiemi (set) delle locazioni di memoria totali diviso la
    dimensione in byte di una parola per le parole in un \textit{set}. Le cache
    sono più piccole della memoria perché sono molto più costose da realizzare:
    \begin{equation*}
        \begin{aligned}
            B = C/b \leq \frac{\text{dim. in byte della memoria}}{4 \cdot b}
        \end{aligned}
    \end{equation*}

    Per questo motivo introduciamo il concetto di \textit{tag}. Il \textit{tag}
    è l'insieme di bit rimanenti da un indirizzo di memoria una volta che
    abbiamo rimosso gli ultimi bit che rappresentano (rispettivamente dal meno
    significativo) il \textit{byte offset}, il numero di parola \textit{word
    number} e il numero di insieme \textit{set number}. Le cache ad accesso
    diretto contengono anche generalmente un bit aggiuntivo nel set, detto $V$,
    che indica se quell'insieme è valido. In una cache ad accesso diretto, per
    accedere ad una locazione di memoria e controllare il caso in cui ci sia un
    \textit{cache hit}, la cache controllerà se al \textit{set number}
    dell'indirizzo richiesto è valido e il tag dell'indirizzo richiesto
    corrisponde al tag memorizzato nella cache. In tale caso, avremo un
    \textit{cache hit}, altrimenti, un \textit{cache miss}.

    \begin{table}[]
        \centering
        \caption{Struttura di un indirizzo di memoria da 32 bit in una cache ad indirizzamento diretto}
        \label{tab:addr-direct}
        \begin{tabular}{|l|l|l|}
        \hline
        Tag & Set & Byte offset \\ \hline
        \end{tabular}
        \end{table}
\end{defn}

\begin{defn}
    \textbf{Cache ad indirizzamento set-associative (N-way)} \\
    Si utilizza una modalità diretta per indirizzare il numero di set, e si
    raggruppano diverse \textit{linee di cache} all'interno del set,
    differenziate per il \textit{tag}. L'indirizzamento per insieme è analogo ad
    una cache ad indirizzamento diretto, solo che un insieme, invece di
    contenere un singolo blocco di cache, ne contiene $N$. Detto in altre parole
    \textit{una cache ad indirizzamento diretto è una cache set-associataive ad
    1 via},  Rispetto ad una cache ad indirizzamento diretto sarà necessario
    introdurre dei confrontatori (per i tag), un decoder per selezionare il
    valore letto dalla memoria e dei decoder. La maggioranza delle cache dei
    processori moderni sono set-associative ad 8 o 16 vie. La struttura di un
    indirizzo per una cache set-associative sarà quindi:
    \begin{table}[htbp]
        \centering
        \caption{Struttura di un indirizzo di memoria da 32 bit in una cache set-associative}
        \label{tab:addr-setassoc}
        \begin{tabular}{|l|l|l|l|}
        \hline
        Tag & Set & Block Offset & Byte offset \\ \hline
        \end{tabular}
    \end{table}
\end{defn}

\begin{defn}
    \textbf{Conflitto} \\
    Quando due indirizzi con accesso effettuato recentemente vengono mappati
    nello stesso blocco di cache, avviene una situazione definita
    "\textbf{conflitto}", dove il blocco più recente "sfratta" il blocco
    precedentemente memorizzato nella cache. Le cache ad indirizzamento diretto
    hanno un solo blocco per ogni set, quindi due indirizzi che mappano allo
    stesso set causano sempre un conflitto.
\end{defn}


\begin{defn}
    \textbf{Cache fully associative} \\
    Una cache fully associative è una cache set-associative a $B$ vie che
    contiene un solo set con $B$ blocchi. Le cache fully associative sono le
    cache con meno miss dovuti a conflitti, fissata una certa capacità. Il loro
    svantaggio è che richiedono molto più hardware per effettuare la
    comparazione dei tag.
\end{defn}


\begin{exmp}
    \textbf{Differenza fra una cache full associative e una cache ad
    indirizzamento diretto} \\

    In una \textbf{cache full associative}, ogni blocco della memoria principale
    può essere posizionato ovunque nella cache. Supponiamo che la dimensione di
    un blocco della cache sia $2n$, per qualche valore $n$ (tipicamente fra 4 e
    6). L'indirizzo di un riferimento alla memoria ha gli $n$ bit meno
    significativi rimossi e il resto dell'indirizzo rappresenta il campo tag.
    Per vedere se il corrispondente blocco fisico di memoria è nella cache,
    vengono confrontati parallelamente i campi tag dei blocchi nella cache.

    Per una \textbf{cache ad indirizzamento diretto}, ogni blocco nella memoria
    può essere associato ad un singolo insieme nella cache. Ugualmente, gli $n$
    bit meno significativi vengono rimossi e il resto dell'indirizzo rappresenta
    il campo tag. La parte rimanente dell'indirizzo viene comparata
    all'indirizzo del blocco della cache per determinare se il blocco è
    memorizzato nella cache.

    In una cache fully associative si riduce il rischio di collisione fra i
    blocchi con i quali si intende occupare la cache. Ovvero, due blocchi di
    memoria che dovrebbero occupare la stessa posizione in una cache ad
    indirizzamento diretto (collisione) potrebbero occupare diversi blocchi in
    una cache full (o set) associative. Ciò richiedede più hardware per
    determinare quale insieme della cache occupare e per cercare in parallelo
    fra tutti i blocchi della cache per determinare se un accesso alla memoria
    produce un \textit{hit}.

    \begin{table}[htbp]
        \centering
        \caption{Tipologie di organizzazione delle cache}
        \label{tab:caches}
        \begin{tabular}{|l|l|l|}
        \hline
        Organizzazione         & Numero di Vie ($N$) & Numero di insiemi ($S$)
        \\ \hline
        Indirizzamento diretto & $1$                 & $B$ \\ \hline
        Set associative        & $1 < N < B$         & $B/N$ \\ \hline
        Fully associative      & $B$                 & 1 \\ \hline
        \end{tabular}
    \end{table}
\end{exmp}


\begin{defn}
    \textbf{Hit rate in relazione alla capacità di una cache} \\
    L'hit rate aumenta se aumenta la capacità in parole della cache (numero
    linee $\times$ numero di parole per ogni linea), mentre il miss rate
    decresce all'aumentare della capacità.
\end{defn}

\begin{defn}
    \textbf{Multipli livelli di cache} \\
    Nei sistemi moderni si utilizzano multipli livelli di cache per ridurre il
    tempo di accesso alla memoria. Cache di dimensioni maggiori sono di utili
    perché mantengono più dati e hanno un miss rate basso. La cache di primo
    livello (L1) è piccola e molto rapida, con un tempo di accesso di circa uno
    o due cicli di clock, la cache di secondo livello (L2) è anch'essa una
    memoria SRAM, ma di dimensioni maggiori e di conseguenza più lenta della
    cache di primo livello.

%    \begin{figure}[htp] \centering \begin{tikzpicture} \coordinate (A) at
%        (-5,0) {}; \coordinate (B) at ( 5,0) {}; \coordinate (C) at (0,5) {};
%        \draw[name path=AC] (A) -- (C); \draw[name path=BC] (B) -- (C);
%        \foreach \y/\A in {0/Cache L1,1/Cache L2,2/Memoria Principale,3 Memoria
%        virtuale} {\path[name path=horiz] (A|-0,\y) -- (B|-0,\y); \draw[name
%        intersections={of=AC and horiz,by=P}, name intersections={of=BC and
%        horiz,by=Q}] (P) -- (Q) node[midway,above] {\A};
%        }
%        \draw[->] (0:0) -- (45:4) node[above] {Capacità}
%        \end{tikzpicture}
%        \end{figure}
\end{defn}



% //TODO Automa parte di controllo, S1: calcolo bit S2: scrivere parole S3:
% leggo parole comune di P //TODO req e ack della parte di controllo

\section{Paging e gestione della memoria}

\begin{defn}
    \textbf{Memoria fisica e problematiche} \\
    La \textbf{memoria fisica} il modulo RAM può avere dimensioni minori
    rispetto al possibile spazio di indirizzamento a 32 bit. Ad esempio, un
    Raspberry Pi con una memoria fisica da 1GB non può utilizzare tutto lo
    spazio di indirizzamento disponibile (4GB). Deve esistere un metodo per
    mappare gli indirizzi disponibile allo spazio realmente disponibile. Un
    altro problema può insorgere quando un programma viene caricato in memoria
    ad un certo indirizzo. Il program counter dovrà riferirsi all'indirizzo
    relativo al programma sommato all'offset di dove viene caricato. Sommare un
    offset a tutti gli indirizzi al caricamento del programma non è efficiente
    ed è una procedura complessa.
\end{defn}

\begin{defn}
    \textbf{Frammentazione di memoria } \\
    La \textbf{frammentazione} è un fenomeno che avviene quando uno spazio di
    memoria viene utilizzato in maniera inefficiente. Avviene quando in memoria
    sono allocati un gran numero di blocchi in maniera non contigua, lasciando
    la maggior parte della memoria non allocata inutilizzabile per la mancanza
    di spazio fra le diverse sezioni allocate.
\end{defn}

\begin{defn}
    \textbf{Spazio di memoria virtuale e memoria fisica} \\
    Un programma in esecuzione può accedere solo ad uno spazio di
    \textbf{memoria virtuale}, un'astrazione per ovviare alle problematiche di
    frammentazione, offset e spazio disponibile. Lo spazio di \textbf{memoria
    fisica} è lo spazio "reale" presente nel modulo RAM.
\end{defn}

\begin{defn}
    \textbf{Pagina} \\
    Una pagina sono $2^k$ posizioni consecutive contigue della memoria fisica,
    utilizzate per rappresentare la memoria virtuale in "pagine" consecutive.
    Iniziano dall'indirizzo dove gli ultimi k bit sono settati a 0. Separiamo
    sia la memoria fisica che la memoria virtuale in pagine.
\end{defn}

\begin{defn}
    \textbf{Working Set} \\
    Il working set è la collezione di pagine di memoria virtuale di "proprietà"
    di un processo in esecuzione.
\end{defn}

\begin{exmp}
    \textbf{Risolvere i problemi di frammentazione, offset e spazio disponibile}
    \\
    Per risolvere il \textbf{problema dello spazio disponibile} si caricano in
    memoria soltanto le pagine che appartengono al working set. Per risolvere il
    \textbf{problema di offset} (dello spazio di memoria virtuale quando una
    pagina viene caricata in uno spazio di memoria fisico) divido l'indirizzo di
    memoria in due sezioni (questa suddivisione è completamente indipendente a
    quella effettuata per la gestione delle cache):

    \begin{table}[htbp]
        \centering
        \caption{Struttura di un indirizzo di memoria per la suddivisione in pagine logiche}
        \label{tab:addr-logicpage}
        \begin{tabular}{|l|l|}
        \hline
        Logic Page Number & Offset ($k$ bit) \\ \hline
        \end{tabular}
    \end{table}

    Per mantenere l'associazione fra pagina di memoria virtuale e pagina di
    memoria fisica il sistema operativo mantiene una tabella (detta di
    rilocazione del processo), dove per ogni processo è associato un numero di
    pagina fisica (NB: se l'associazione è presente!) al numero di pagina
    virtuale. Un indirizzo relativo allo spazio di memoria virtuale viene detto
    \textbf{indirizzo logico}, per accedere alla memoria fisica, ogni volta che
    devo accedere ad un indirizzo logico, viene recuperato dalla tabella il
    numero di pagina fisica e viene sostituito al numero di pagina logica per
    produrre un indirizzo fisico.

    Il \textbf{il problema di frammentazione} è automaticamente risolto
    introducendo le pagine, perché esse hanno tutte dimensione uguale. La
    contiguità delle posizioni di memoria, anche se nello spazio fisico le
    pagine non sono realmente contigue, è garantito dalla conversione fra
    indirizzo logico ed indirizzo fisico.
\end{exmp}

\begin{defn}
    \textbf{MMU} \\
    La Memory Management Unit è un componente fisico, spesso incluso insieme al
    processore, che ha il compito di mantenere un sottoinsieme la tabella di
    paginazione ed effettuare la conversione da indirizzo logico ad indirizzo
    fisico. È posta fra il processore (che invia un operazione di read, write o
    store con un indirizzo logico e un valore opzionale) e la memoria (che
    restituisce direttamente al processore il dato richiesto). In caso di fault,
    la MMU si occuperà di notificare il processore. La conversione fra indirizzi
    logici e fisici avviene in maniera totalmente trasparente al processore. La
    MMU contiene un sottoinsieme (in maniera sostanzialmente analoga ad una
    cache completamente associativa) della tabella di associazioni per ogni
    processo fra indirizzo di pagina logica $\rightarrow$ indirizzo di pagina
    fisica. La MMU può essere interna al circuito integrato del processore,
    oppure un componente esterno.
\end{defn}

\begin{defn}
    \textbf{Paginazione dinamica} \\
    La paginazione dinamica è una policy di paginazione dove vediamo la memoria
    fisica come se fosse una "\textit{cache dello spazio di memoria virtuale}"
    (presente sulla memoria di massa e di conseguenza molto più ampio). Al
    momento dell'esecuzione di un programma viene caricata in memoria fisica
    soltanto la pagina contenente la sezione di codice \texttt{.text} contenente
    (nella maggior parte dei casi) una procedura \texttt{.main}. Al momento
    dell'accesso alla memoria da parte del programma caricato la MMU effettuerà
    la conversione dell'indirizzo al cui si intende accedere, convertendo il
    numero di pagina logica ad un numero di pagina fisica, memorizzando
    l'associazione nella \textbf{tabella di rilocazione del processo}, che oltre
    a contenenere il numero di pagina fisica corrispondente alla pagina logica,
    conterrà anche un \textit{bit di presenza}, che indica se la pagina logica è
    attualmente caricata in quella fisica associata. Se cerchiamo di accedere ad
    una pagina logica a cui è associata una pagina fisica \textbf{ma il bit di
    presenza} è impostato a 0, la MMU restituirà un'eccezione che verrà gestita
    dal sistema operativo in modo tale che esso restituisca una pagina della
    memoria fisica \textbf{libera}. Una volta trovata una pagina fisica libera
    essa viene associata alla pagina logica nella tabella di rilocazione, la
    pagina logica viene caricata ed il bit di presenza viene impostato ad 1. Se
    non vi è abbastanza spazio nella memoria fisica si utilizza generalmente una
    politica LRU (Least Recently Used). Tali eccezioni vengonono chiamate
    \textbf{fault di pagina}.
\end{defn}

\begin{figure}[H]
    \centering

    \caption{Automa (semplificato) dell parte di controllo di una MMU}
    \label{fig:pcmmu}
	\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=6cm]

    \node[state,initial] (A) {\makecell[l]{logical\\address}};
    \node[state] (B) [below left of=A] {\makecell[l]{physical\\address}};
    \node[state] (C) [right of=A] {\makecell[l]{memory access\\(find victim\\and replace)}};
    \node[state] (G) [below of=C] {\makecell[l]{page fault}};

    \path
    (A) edge [bend left] node [] {\makecell[l]{cache hit}} (B)
        edge [bend left] node [] {\makecell[l]{cache miss}}  (C)
    (B) edge [bend left] node [] {} (A)
    (C) edge [] node [below] {\makecell[l]{$P = 1$\\page is present}} (A)
        edge [] node [] {$P = 0$} (G)
    (G) edge [bend left] node [] {} (A);
    \end{tikzpicture}
    % TODO fixa label
\end{figure}

\begin{defn}
    \textbf{TLB} \\
    Il Transition Lookaside Buffer (TLB) è una cache che l'MMU utilizza per
    velocizzare la traduzione degli indirizzi virtuali ad indirizzi fisici. Il
    TLB possiede un numero fisso di elementi della \textbf{tabella di
    rilocazione del processo} (Page Table). È una memoria chiave-valore
    indirizzata per contenuto che associa un indirizzo virtuale ad un indirizzo
    fisico. Nel processore BCM2835 è presente una gerarchia di TLB: i micro TLB
    ($\mu$TLB), due buffer da 10 posizioni uno per istruzioni e uno per dati
    che impiegano un singolo ciclo di clock per l'accesso con una policy di
    rimpiazzamento \textbf{Round Robin}, ed è presente anche un TLB generale
    unificato per istruzioni e dati, ovvero una cache set associativa nella
    quale la ricerca viene eseguita solo quando si ha un miss nei micro TLB.
    Se anche il TLB generale restituisce un miss, significa che la Page Table
    della MMU non contiene la traduzione da logico a fisico dell'indirizzo
    richiesto e si propaga la richiesta alla memoria.
\end{defn}


%\begin{defn} \textbf{Tipi di Fault} \\
%\end{defn}

% TODO lezione 31 TODO lezione 32